<html>
<head>
<title>KG-BIAS Workshop 2020</title>
</head>
<body>
<h1>KG-BIAS Workshop 2020</h1>

Editors:<br />
Edgar Meij (Bloomberg)<br />
Tara Safavi (Uni of Michigan)<br />
Chenyan Xiong (Microsoft Research)<br />
Gianluca Demartini (Uni of Queensland)<br />
Miriam Redi (Wikimedia Foundation)<br />
Fatma &#214;zcan (IBM Research)<br /><br />
Virtual event, June 25, 2020<br /><br />

The KG-BIAS 2020 workshop touches on biases and how they surface in knowledge graphs (KGs), biases in the source data that is used to create KGs, methods for measuring or remediating bias in KGs, but also identifying other biases such as how and which languages are represented in automatically constructed KGs or how personal KGs might incur inherent biases. The goal of this workshop is to uncover how various types of biases are introduced into KGs, investigate how to measure, and propose methods to remediate them.<br /><br />
  
<dl>
<dt><b>Preface</b></dt>
<dd>Edgar Meij, Tara Safavi, Chenyan Xiong, Gianluca Demartini, Miriam Redi, Fatma &#214;zcan<br />
Knowledge graphs (KGs) store human knowledge about the world in structured format (e.g., triples of facts or graphs of entities and relations) to be processed by various artificial intelligence systems. In the past decade, extensive research efforts have gone into constructing and utilizing knowledge graphs for tasks in natural language processing, information retrieval, recommender systems, and more. Once constructed, knowledge graphs are often considered as "gold standard" data sources that safeguard the correctness of other systems. Because the biases inherent to KGs may become magnified and spread through such systems, it is crucial that we acknowledge and address various types of bias in knowledge graph construction.<br /><br />
This workshop, held for the first time in conjunction with the conference on Automated Knowledge Base Construction (AKBC), provides a platform for systematically investigating bias in all aspects of automatic knowledge graph construction. Such biases may originate in the data used to construct or curate knowledge graphs, and may also be present in the downstream algorithms and systems that rely on knowledge graphs. Types of data bias in KGs include those pertaining to geography, gender, and language; knowledge graphs can even be biased by interest, since the criteria for including information in a knowledge graph relies heavily on subjective definitions of "noteworthiness". Algorithms and systems may amplify these biases or introduce new ones.<br /><br />
In keeping with the theme of the conference, our program is interdisciplinary. We received and accepted three submissions on (1) fairness and transparency in knowledge graph construction; (2) demographic bias in named entity recognition systems; (3) the pitfalls of personalization in conversational search. We also invited one paper that addresses how knowledge graph embeddings can amplify social biases. Finally, we are excited to have two keynote speakers: Jahna Otterbacher from the Open University of Cyprus, speaking on algorithmic and data biases across several domains of computer science, and Jieyu Zhao from the University of California, Los Angeles, speaking on gender bias in natural language processing.<br /><br />
Broadly, our hope is to foster collaboration and agree on a shared research agenda by bringing together and growing a community of researchers from a diverse set of communities across academia, industry, and nonprofits around this important but understudied topic.<br /><br />
</dd>
</dl>

<dl>
<dt><b>From Knowledge Graphs to Knowledge Practices: On the Need for Transparency and Explainability in Enterprise Knowledge Graph Applications.</b></dt>
<dd>Christine Wolf<br />
(paper 
REPORT-NO:KGBias/2020/01
)
</dd>
</dl>

<dl>
<dt><b>Assessing Demographic Bias in Named Entity Recognition.</b></dt>
<dd>Shubhanshu Mishra, Sijun He, Luca Belli<br />
(paper 
REPORT-NO:KGBias/2020/02
)
</dd>
</dl>

<dl>
<dt><b>Bias in Conversational Search: The Double-Edged Sword of the Personalized Knowledge Graph</b></dt>
<dd>Emma Gerritse, Faegheh Hasibi, Arjen P. de Vries<br />
(paper not included in the proceedings)
</dd>
</dl>

<dl>
<dt><b>Measuring social bias in knowledge graph embeddings</b></dt>
<dd>Joseph Fisher, Dave Palfrey, Arpit Mittal, Christos Christodoulopoulos<br />
(paper 
REPORT-NO:KGBias/2020/03
)
</dd>
</dl>

</body>
</html>


